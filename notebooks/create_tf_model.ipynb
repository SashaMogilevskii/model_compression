{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import time\n",
    "import datetime\n",
    "import warnings\n",
    "import sys\n",
    "sys.path.append(\"../scr\")\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import timm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "from box import Box\n",
    "from torch.utils.data import DataLoader\n",
    "from loguru import logger\n",
    "\n",
    "from utils.create_dataset import BirdDataset\n",
    "from utils.base_utils import set_seed\n",
    "from utils.metrics import validation_epoch_end\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-05 15:54:59.370\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mFolder with experiment- ../experiment\\05_October_2023_15_54\u001b[0m\n",
      "\u001b[32m2023-10-05 15:54:59.371\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m----------params----------\u001b[0m\n",
      "\u001b[32m2023-10-05 15:54:59.373\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mdebug: False\u001b[0m\n",
      "\u001b[32m2023-10-05 15:54:59.374\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mseed: 1771\u001b[0m\n",
      "\u001b[32m2023-10-05 15:54:59.375\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mpath_to_files_base: ../data\u001b[0m\n",
      "\u001b[32m2023-10-05 15:54:59.377\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mbatch_size: 16\u001b[0m\n",
      "\u001b[32m2023-10-05 15:54:59.378\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mnum_workers: 0\u001b[0m\n",
      "\u001b[32m2023-10-05 15:54:59.379\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1moptimizer_lr: 0.006\u001b[0m\n",
      "\u001b[32m2023-10-05 15:54:59.380\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1moptimizer_wd: 0\u001b[0m\n",
      "\u001b[32m2023-10-05 15:54:59.381\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mmodel_name: tf_efficientnet_b0\u001b[0m\n",
      "\u001b[32m2023-10-05 15:54:59.382\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mmetric: custom\u001b[0m\n",
      "\u001b[32m2023-10-05 15:54:59.383\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mepochs: 4\u001b[0m\n",
      "\u001b[32m2023-10-05 15:54:59.667\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.base_utils\u001b[0m:\u001b[36mset_seed\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mSet seed: 1771\u001b[0m\n",
      "\u001b[32m2023-10-05 15:54:59.698\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSize df_train- 12326\u001b[0m\n",
      "\u001b[32m2023-10-05 15:54:59.699\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m33\u001b[0m - \u001b[1mSize df_test- 3082\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "date_now = datetime.datetime.now().strftime(\"%d_%B_%Y_%H_%M\")\n",
    "\n",
    "path_save = os.path.join(\"../experiment\", date_now)\n",
    "if not os.path.exists(path_save):\n",
    "    os.makedirs(path_save)\n",
    "\n",
    "# Load config with params for training\n",
    "with open(\"../scr/config_tf.yaml\", \"r\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "    config = Box(config)\n",
    "\n",
    "logger.add(f\"{path_save}/info_log{date_now}.log\",\n",
    "           format=\"<red>{time:YYYY-MM-DD HH:mm:ss}</red>| {message}\")\n",
    "\n",
    "\n",
    "logger.info(f\"Folder with experiment- {path_save}\")\n",
    "logger.info(\"----------params----------\")\n",
    "\n",
    "for param in config:\n",
    "    logger.info(f\"{param}: {str(config[param])}\")\n",
    "\n",
    "# Create device for training and set_seed:\n",
    "config.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "set_seed(seed=config.seed)\n",
    "\n",
    "# Read data\n",
    "df = pd.read_csv(\"../data/data.csv\")\n",
    "df_train, df_test = (df[df.fold != 3].reset_index(drop=True),\n",
    "                     df[df.fold == 3].reset_index(drop=True)\n",
    "                     )\n",
    "logger.info(f\"Size df_train- {df_train.shape[0]}\")\n",
    "logger.info(f\"Size df_test- {df_test.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_train = BirdDataset(df=df_train,\n",
    "                            path_to_folder_with_audio=config.path_to_files_base,\n",
    "                            tensorflow=True\n",
    "                            )\n",
    "dataset_test = BirdDataset(df=df_test,\n",
    "                           path_to_folder_with_audio=config.path_to_files_base,\n",
    "                           tensorflow=True\n",
    "                           )\n",
    "\n",
    "train_loader = DataLoader(dataset_train,\n",
    "                          batch_size=config.batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=config.num_workers)\n",
    "valid_loader = DataLoader(dataset_test,\n",
    "                          batch_size=config.batch_size,\n",
    "                          num_workers=config.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.applications.efficientnet.EfficientNetB0(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(626, 256, 3),\n",
    "    drop_connect_rate=0.4,\n",
    ")\n",
    "\n",
    "input_shape = (626, 256, 3)\n",
    "# Define the pre-processing layers for the input spectrogram\n",
    "# preprocessing_layers = [\n",
    "#     preprocessing.Normalization(),\n",
    "# ]\n",
    "\n",
    "# Define the spectrogram classification model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=input_shape),\n",
    "    # *preprocessing_layers,\n",
    "    model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(264, activation='sigmoid')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=config.optimizer_lr,\n",
    "    decay_steps=config.epochs * len(train_loader),  # Устанавливаем decay_steps равным общему числу эпох\n",
    "    alpha=0.0  # alpha устанавливается в 0.0 для чистой косинусной функции\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.BinaryCrossentropy()  # Для бинарной классификации\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получите текущее значение learning rate\n",
    "current_learning_rate = optimizer.learning_rate.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model\n",
    "metric = validation_epoch_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-05 15:55:08.779\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[1mStarting train. Model - tf_efficientnet_b0\u001b[0m\n",
      "\u001b[32m2023-10-05 15:55:08.782\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1m---------------------epoch:1/4---------------------\u001b[0m\n",
      "Training: 100%|██████████| 771/771 [2:28:23<00:00, 11.55s/it]  \n",
      "Validation: 100%|██████████| 193/193 [11:40<00:00,  3.63s/it]\n",
      "\u001b[32m2023-10-05 18:35:14.065\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1mepoch: 1\u001b[0m\n",
      "\u001b[32m2023-10-05 18:35:14.108\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mloss_train: 18.7400| loss_valid: 0.0000|\u001b[0m\n",
      "\u001b[32m2023-10-05 18:35:14.109\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m71\u001b[0m - \u001b[1mmetric val_RMAP : 0.107474\u001b[0m\n",
      "\u001b[32m2023-10-05 18:35:14.111\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m77\u001b[0m - \u001b[1mElapsed time: 02:40:05\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../experiment\\05_October_2023_15_54/model_tf_efficientnet_b0_ep_1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../experiment\\05_October_2023_15_54/model_tf_efficientnet_b0_ep_1\\assets\n",
      "\u001b[32m2023-10-05 18:35:39.957\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1m---------------------epoch:2/4---------------------\u001b[0m\n",
      "Training: 100%|██████████| 771/771 [2:26:24<00:00, 11.39s/it]  \n",
      "Validation: 100%|██████████| 193/193 [11:31<00:00,  3.59s/it]\n",
      "\u001b[32m2023-10-05 21:13:36.886\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1mepoch: 2\u001b[0m\n",
      "\u001b[32m2023-10-05 21:13:36.900\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mloss_train: 13.9301| loss_valid: 0.0000|\u001b[0m\n",
      "\u001b[32m2023-10-05 21:13:36.901\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m71\u001b[0m - \u001b[1mmetric val_RMAP : 0.157993\u001b[0m\n",
      "\u001b[32m2023-10-05 21:13:36.902\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m77\u001b[0m - \u001b[1mElapsed time: 02:37:56\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../experiment\\05_October_2023_15_54/model_tf_efficientnet_b0_ep_2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../experiment\\05_October_2023_15_54/model_tf_efficientnet_b0_ep_2\\assets\n",
      "\u001b[32m2023-10-05 21:14:01.236\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1m---------------------epoch:3/4---------------------\u001b[0m\n",
      "Training: 100%|██████████| 771/771 [2:24:14<00:00, 11.22s/it]  \n",
      "Validation: 100%|██████████| 193/193 [13:54<00:00,  4.32s/it]\n",
      "\u001b[32m2023-10-05 23:52:10.404\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1mepoch: 3\u001b[0m\n",
      "\u001b[32m2023-10-05 23:52:10.445\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mloss_train: 11.4857| loss_valid: 0.0000|\u001b[0m\n",
      "\u001b[32m2023-10-05 23:52:10.446\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m71\u001b[0m - \u001b[1mmetric val_RMAP : 0.190731\u001b[0m\n",
      "\u001b[32m2023-10-05 23:52:10.448\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m77\u001b[0m - \u001b[1mElapsed time: 02:38:09\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../experiment\\05_October_2023_15_54/model_tf_efficientnet_b0_ep_3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../experiment\\05_October_2023_15_54/model_tf_efficientnet_b0_ep_3\\assets\n",
      "\u001b[32m2023-10-05 23:52:44.452\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1m---------------------epoch:4/4---------------------\u001b[0m\n",
      "Training:  25%|██▍       | 189/771 [42:49<2:05:53, 12.98s/it]"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Starting train. Model - {config.model_name}\")\n",
    "for epoch_i in range(1, config.epochs + 1):\n",
    "    if config.debug:\n",
    "        k = 1\n",
    "    start = time.time()\n",
    "    logger.info(f'---------------------epoch:{epoch_i}/{config.epochs}---------------------')\n",
    "\n",
    "    # loss\n",
    "    avg_train_loss = 0\n",
    "    avg_val_loss = 0\n",
    "    predicted_labels_list = None\n",
    "    true_labels_list = None\n",
    "\n",
    "    ############## Train #############\n",
    "    train_pbar = tqdm(train_loader, desc=\"Training\")\n",
    "    for batch in train_pbar:\n",
    "        X_batch = batch[0]\n",
    "        y_batch = batch[1]\n",
    "\n",
    "        #convert to tf \n",
    "        X_batch = tf.convert_to_tensor(X_batch.numpy(), dtype=tf.float32)\n",
    "        y_batch = tf.convert_to_tensor(y_batch.numpy(), dtype=tf.float32)\n",
    "\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(X_batch, training=True)\n",
    "            loss_value = loss_fn(y_batch, logits)\n",
    "\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        avg_train_loss += loss_value.numpy()\n",
    "\n",
    "        if config.debug:\n",
    "            k += 1\n",
    "            if k > 3: break\n",
    "\n",
    "    valid_pbar = tqdm(valid_loader, desc=\"Validation\")\n",
    "    for batch in (valid_pbar):\n",
    "        X_batch = batch[0]\n",
    "        y_b = batch[1]\n",
    "\n",
    "        #convert to tf \n",
    "        X_batch = tf.convert_to_tensor(X_batch.numpy(), dtype=tf.float32)\n",
    "        y_batch = tf.convert_to_tensor(y_b.numpy(), dtype=tf.float32)\n",
    "\n",
    "        logits = model(X_batch, training=True)\n",
    "        loss_value = loss_fn(y_batch, logits)\n",
    "        if predicted_labels_list is None:\n",
    "                predicted_labels_list = logits\n",
    "                true_labels_list = y_b\n",
    "        else:\n",
    "            predicted_labels_list = np.concatenate([predicted_labels_list, logits.numpy()], axis=0)\n",
    "            true_labels_list = np.concatenate([true_labels_list, y_b], axis=0)\n",
    "            \n",
    "        if config.debug:\n",
    "            k += 1\n",
    "            if k > 6:  break\n",
    "\n",
    "\n",
    "    all_predicted_labels = np.vstack(predicted_labels_list)\n",
    "    all_true_labels = np.vstack(true_labels_list)\n",
    "    all_true_labels = np.squeeze(all_true_labels)\n",
    "    mask = (all_true_labels > 0) & (all_true_labels < 1)\n",
    "    all_true_labels[mask] = 0\n",
    "    avg_metric = metric(all_true_labels, all_predicted_labels)\n",
    "\n",
    "    logger.info(f'epoch: {epoch_i}')\n",
    "\n",
    "    logger.info(\"loss_train: %0.4f| loss_valid: %0.4f|\" % (avg_train_loss, avg_val_loss))\n",
    "    for m in avg_metric:\n",
    "        logger.info(f\"metric {m} : {avg_metric[m]:.<5g}\")\n",
    "\n",
    "    elapsed_time = time.time() - start\n",
    "    hours = int(elapsed_time // 3600)\n",
    "    minutes = int((elapsed_time % 3600) // 60)\n",
    "    seconds = int(elapsed_time % 60)\n",
    "    logger.info(f\"Elapsed time: {hours:02d}:{minutes:02d}:{seconds:02d}\")\n",
    "    tf.keras.saving.save_model(model, f'{path_save}/model_{config.model_name}_ep_{epoch_i}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
