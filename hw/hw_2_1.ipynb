{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T18:19:12.606923Z",
     "start_time": "2023-09-17T18:19:09.928171Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../scr/utils\")\n",
    "\n",
    "\n",
    "from copy import deepcopy\n",
    "import time\n",
    "import platform\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from create_dataset import BirdDataset\n",
    "from base_utils import set_seed\n",
    "from metrics import validation_epoch_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T18:19:12.612994Z",
     "start_time": "2023-09-17T18:19:12.607913Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Информация о процессоре: arm\n",
      "GPU недоступны на данной системе.\n"
     ]
    }
   ],
   "source": [
    "# Информация о железе, на котором тестируется модель\n",
    "\n",
    "processor_info = platform.processor()\n",
    "print(\"Информация о процессоре:\", processor_info)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(\"Доступно GPU:\", num_gpus)\n",
    "    for i in range(num_gpus):\n",
    "        gpu = torch.cuda.get_device_name(i)\n",
    "        print(f\"GPU {i + 1}: {gpu}\")   \n",
    "else:\n",
    "    print(\"GPU недоступны на данной системе.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T18:19:12.976431Z",
     "start_time": "2023-09-17T18:19:12.612765Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cpu')"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/data.csv\")\n",
    "df_test = df[df.fold == 3].sample(n=100, random_state=42).reset_index(drop=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_base = torch.load(\"../experiment/14_September_2023_16_29/model_tf_efficientnet_b0_last_version.pt\", map_location=device).to(device)\n",
    "dataset_test = BirdDataset(df=df_test, path_to_folder_with_audio=\"../data\")\n",
    "valid_loader = DataLoader(dataset_test, batch_size=4)\n",
    "model_base.eval()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Проверим работу базовой модели"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T18:19:12.994244Z",
     "start_time": "2023-09-17T18:19:12.972095Z"
    }
   },
   "outputs": [],
   "source": [
    "model = deepcopy(model_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_model_size(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    print('model size: {:.3f}MB'.format(size_all_mb))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T18:19:13.017890Z",
     "start_time": "2023-09-17T18:19:12.994998Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T18:19:13.026981Z",
     "start_time": "2023-09-17T18:19:13.002841Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_metric_score(model):\n",
    "    predicted_labels_list = None\n",
    "    true_labels_list = None\n",
    "    metric = validation_epoch_end\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_loader):\n",
    "            X_batch = batch[0].to(device)\n",
    "            y_batch = batch[1].to(device)\n",
    "            res = model.forward(X_batch)\n",
    "\n",
    "            res = res.detach().sigmoid().cpu().numpy()\n",
    "            y_batch_onehot = y_batch\n",
    "            y_batch_onehot = y_batch_onehot.unsqueeze(1).detach().cpu().numpy()\n",
    "            y_batch_onehot = y_batch_onehot.squeeze()\n",
    "\n",
    "            if predicted_labels_list is None:\n",
    "                predicted_labels_list = res\n",
    "                true_labels_list = y_batch_onehot\n",
    "            else:\n",
    "                predicted_labels_list = np.concatenate([predicted_labels_list, res], axis=0)\n",
    "                true_labels_list = np.concatenate([true_labels_list, y_batch_onehot], axis=0)\n",
    "\n",
    "            del batch, res\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "\n",
    "    all_predicted_labels = np.vstack(predicted_labels_list)\n",
    "    all_true_labels = np.vstack(true_labels_list)\n",
    "    all_true_labels = np.squeeze(all_true_labels)\n",
    "    mask = (all_true_labels > 0) & (all_true_labels < 1)\n",
    "    all_true_labels[mask] = 0\n",
    "    avg_metric = metric(all_true_labels, all_predicted_labels)\n",
    "    t = end_time - start_time\n",
    "\n",
    "    # Мы понимаем, что ко времени работы модели добавляем время обработки батчей и добавления аугментаций к стартовым данным\n",
    "\n",
    "    print(\"Наши метрики на нашей базовой модели:\")\n",
    "    print(f\"Время работы модели на всем батче {t:.<2g} сек.\")\n",
    "    print(f\"Время работы модели на одном сэмпле (AVG) {round(t/ len(dataset_test), 3) } сек.\")\n",
    "    print(\"Метрики качества:\")\n",
    "    for m in avg_metric:\n",
    "        print(f\"metric {m} : {avg_metric[m]:.<5g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 16.738MB\n"
     ]
    }
   ],
   "source": [
    "get_model_size(model=model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T18:19:13.027468Z",
     "start_time": "2023-09-17T18:19:13.008865Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T18:19:39.120286Z",
     "start_time": "2023-09-17T18:19:13.012275Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:26<00:00,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наши метрики на нашей базовой модели:\n",
      "Время работы модели на всем батче 26.0951 сек.\n",
      "Время работы модели на одном сэмпле (AVG) 0.261 сек.\n",
      "Метрики качества:\n",
      "metric val_RMAP : 0.724241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "get_metric_score(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PTDQ  fp32 -> qint8"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "model = deepcopy(model_base)\n",
    "torch.backends.quantized.engine = 'qnnpack'\n",
    "model_int8 = torch.ao.quantization.quantize_dynamic(\n",
    "    model,  # the original model\n",
    "    {torch.nn.Linear},  # a set of layers to dynamically quantize\n",
    "    dtype=torch.qint8  # the target dtype for quantized weights\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T18:19:39.174570Z",
     "start_time": "2023-09-17T18:19:39.119989Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 15.448MB\n"
     ]
    }
   ],
   "source": [
    "get_model_size(model=model_int8)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T18:19:39.180282Z",
     "start_time": "2023-09-17T18:19:39.177322Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s][W qlinear_dynamic.cpp:247] Warning: Currently, qnnpack incorrectly ignores reduce_range when it is set to true; this may change in a future release. (function operator())\n",
      "100%|██████████| 25/25 [00:23<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наши метрики на нашей базовой модели:\n",
      "Время работы модели на всем батче 23.5511 сек.\n",
      "Время работы модели на одном сэмпле (AVG) 0.236 сек.\n",
      "Метрики качества:\n",
      "metric val_RMAP : 0.709696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "get_metric_score(model=model_int8)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T18:20:02.742307Z",
     "start_time": "2023-09-17T18:19:39.180700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Незначительное уменьшение веса модели и метрики"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PTDS  fp32 -> qint8"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "model = deepcopy(model_base)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T18:46:37.262881Z",
     "start_time": "2023-09-17T18:46:37.235438Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "model.eval();"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T18:46:37.476181Z",
     "start_time": "2023-09-17T18:46:37.470919Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "torch.quantization.prepare(model, inplace=True);"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T18:46:37.779693Z",
     "start_time": "2023-09-17T18:46:37.737557Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "sample = iter(valid_loader).__next__()[0]\n",
    "res = model(sample);"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T18:46:47.128321Z",
     "start_time": "2023-09-17T18:46:45.621236Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "EfficientNet(\n  (conv_stem): Conv2dSame(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n  (bn1): BatchNormAct2d(\n    32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n    (drop): Identity()\n    (act): SiLU(inplace=True)\n  )\n  (blocks): Sequential(\n    (0): Sequential(\n      (0): DepthwiseSeparableConv(\n        (conv_dw): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=1.5549452304840088, zero_point=79, padding=(1, 1), groups=32)\n        (bn1): BatchNormAct2d(\n          32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (se): SqueezeExcite(\n          (conv_reduce): QuantizedConv2d(32, 8, kernel_size=(1, 1), stride=(1, 1), scale=0.25679948925971985, zero_point=127)\n          (act1): SiLU(inplace=True)\n          (conv_expand): QuantizedConv2d(8, 32, kernel_size=(1, 1), stride=(1, 1), scale=0.02107876166701317, zero_point=20)\n          (gate): Sigmoid()\n        )\n        (conv_pw): QuantizedConv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), scale=1.1824504137039185, zero_point=61)\n        (bn2): BatchNormAct2d(\n          16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): Identity()\n        )\n        (drop_path): Identity()\n      )\n    )\n    (1): Sequential(\n      (0): InvertedResidual(\n        (conv_pw): QuantizedConv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), scale=4.387275695800781, zero_point=58)\n        (bn1): BatchNormAct2d(\n          96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (conv_dw): Conv2dSame(96, 96, kernel_size=(3, 3), stride=(2, 2), groups=96, bias=False)\n        (bn2): BatchNormAct2d(\n          96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (se): SqueezeExcite(\n          (conv_reduce): QuantizedConv2d(96, 4, kernel_size=(1, 1), stride=(1, 1), scale=0.5399010181427002, zero_point=23)\n          (act1): SiLU(inplace=True)\n          (conv_expand): QuantizedConv2d(4, 96, kernel_size=(1, 1), stride=(1, 1), scale=0.48841795325279236, zero_point=31)\n          (gate): Sigmoid()\n        )\n        (conv_pwl): QuantizedConv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), scale=5.303088188171387, zero_point=55)\n        (bn3): BatchNormAct2d(\n          24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): Identity()\n        )\n        (drop_path): Identity()\n      )\n      (1): InvertedResidual(\n        (conv_pw): QuantizedConv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), scale=4.214626312255859, zero_point=64)\n        (bn1): BatchNormAct2d(\n          144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (conv_dw): QuantizedConv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), scale=0.9026620388031006, zero_point=95, padding=(1, 1), groups=144)\n        (bn2): BatchNormAct2d(\n          144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (se): SqueezeExcite(\n          (conv_reduce): QuantizedConv2d(144, 6, kernel_size=(1, 1), stride=(1, 1), scale=0.24693255126476288, zero_point=46)\n          (act1): SiLU(inplace=True)\n          (conv_expand): QuantizedConv2d(6, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.38306570053100586, zero_point=61)\n          (gate): Sigmoid()\n        )\n        (conv_pwl): QuantizedConv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), scale=0.9357476234436035, zero_point=61)\n        (bn3): BatchNormAct2d(\n          24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): Identity()\n        )\n        (drop_path): Identity()\n      )\n    )\n    (2): Sequential(\n      (0): InvertedResidual(\n        (conv_pw): QuantizedConv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), scale=6.5457282066345215, zero_point=62)\n        (bn1): BatchNormAct2d(\n          144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (conv_dw): Conv2dSame(144, 144, kernel_size=(5, 5), stride=(2, 2), groups=144, bias=False)\n        (bn2): BatchNormAct2d(\n          144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (se): SqueezeExcite(\n          (conv_reduce): QuantizedConv2d(144, 6, kernel_size=(1, 1), stride=(1, 1), scale=0.2161555141210556, zero_point=127)\n          (act1): SiLU(inplace=True)\n          (conv_expand): QuantizedConv2d(6, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.024878477677702904, zero_point=64)\n          (gate): Sigmoid()\n        )\n        (conv_pwl): QuantizedConv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), scale=0.9718902707099915, zero_point=69)\n        (bn3): BatchNormAct2d(\n          40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): Identity()\n        )\n        (drop_path): Identity()\n      )\n      (1): InvertedResidual(\n        (conv_pw): QuantizedConv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), scale=6.746982097625732, zero_point=61)\n        (bn1): BatchNormAct2d(\n          240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (conv_dw): QuantizedConv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), scale=1.1298234462738037, zero_point=72, padding=(2, 2), groups=240)\n        (bn2): BatchNormAct2d(\n          240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (se): SqueezeExcite(\n          (conv_reduce): QuantizedConv2d(240, 10, kernel_size=(1, 1), stride=(1, 1), scale=0.14996854960918427, zero_point=61)\n          (act1): SiLU(inplace=True)\n          (conv_expand): QuantizedConv2d(10, 240, kernel_size=(1, 1), stride=(1, 1), scale=0.191707044839859, zero_point=71)\n          (gate): Sigmoid()\n        )\n        (conv_pwl): QuantizedConv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), scale=0.579322874546051, zero_point=62)\n        (bn3): BatchNormAct2d(\n          40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): Identity()\n        )\n        (drop_path): Identity()\n      )\n    )\n    (3): Sequential(\n      (0): InvertedResidual(\n        (conv_pw): QuantizedConv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), scale=7.803716659545898, zero_point=56)\n        (bn1): BatchNormAct2d(\n          240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (conv_dw): Conv2dSame(240, 240, kernel_size=(3, 3), stride=(2, 2), groups=240, bias=False)\n        (bn2): BatchNormAct2d(\n          240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (se): SqueezeExcite(\n          (conv_reduce): QuantizedConv2d(240, 10, kernel_size=(1, 1), stride=(1, 1), scale=1.6556528806686401, zero_point=48)\n          (act1): SiLU(inplace=True)\n          (conv_expand): QuantizedConv2d(10, 240, kernel_size=(1, 1), stride=(1, 1), scale=0.9854979515075684, zero_point=61)\n          (gate): Sigmoid()\n        )\n        (conv_pwl): QuantizedConv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), scale=1.4319533109664917, zero_point=67)\n        (bn3): BatchNormAct2d(\n          80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): Identity()\n        )\n        (drop_path): Identity()\n      )\n      (1): InvertedResidual(\n        (conv_pw): QuantizedConv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), scale=8.584409713745117, zero_point=66)\n        (bn1): BatchNormAct2d(\n          480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (conv_dw): QuantizedConv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), scale=0.37665972113609314, zero_point=69, padding=(1, 1), groups=480)\n        (bn2): BatchNormAct2d(\n          480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (se): SqueezeExcite(\n          (conv_reduce): QuantizedConv2d(480, 20, kernel_size=(1, 1), stride=(1, 1), scale=0.32283473014831543, zero_point=55)\n          (act1): SiLU(inplace=True)\n          (conv_expand): QuantizedConv2d(20, 480, kernel_size=(1, 1), stride=(1, 1), scale=0.6347560882568359, zero_point=70)\n          (gate): Sigmoid()\n        )\n        (conv_pwl): QuantizedConv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), scale=0.9349180459976196, zero_point=60)\n        (bn3): BatchNormAct2d(\n          80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): Identity()\n        )\n        (drop_path): Identity()\n      )\n      (2): InvertedResidual(\n        (conv_pw): QuantizedConv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), scale=9.585262298583984, zero_point=59)\n        (bn1): BatchNormAct2d(\n          480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (conv_dw): QuantizedConv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), scale=0.410649836063385, zero_point=66, padding=(1, 1), groups=480)\n        (bn2): BatchNormAct2d(\n          480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (se): SqueezeExcite(\n          (conv_reduce): QuantizedConv2d(480, 20, kernel_size=(1, 1), stride=(1, 1), scale=0.44203466176986694, zero_point=74)\n          (act1): SiLU(inplace=True)\n          (conv_expand): QuantizedConv2d(20, 480, kernel_size=(1, 1), stride=(1, 1), scale=0.6885343194007874, zero_point=73)\n          (gate): Sigmoid()\n        )\n        (conv_pwl): QuantizedConv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), scale=1.1988098621368408, zero_point=64)\n        (bn3): BatchNormAct2d(\n          80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): Identity()\n        )\n        (drop_path): Identity()\n      )\n    )\n    (4): Sequential(\n      (0): InvertedResidual(\n        (conv_pw): QuantizedConv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), scale=10.163562774658203, zero_point=63)\n        (bn1): BatchNormAct2d(\n          480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (conv_dw): QuantizedConv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), scale=1.0396345853805542, zero_point=76, padding=(2, 2), groups=480)\n        (bn2): BatchNormAct2d(\n          480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (se): SqueezeExcite(\n          (conv_reduce): QuantizedConv2d(480, 20, kernel_size=(1, 1), stride=(1, 1), scale=2.045675039291382, zero_point=27)\n          (act1): SiLU(inplace=True)\n          (conv_expand): QuantizedConv2d(20, 480, kernel_size=(1, 1), stride=(1, 1), scale=1.8779391050338745, zero_point=59)\n          (gate): Sigmoid()\n        )\n        (conv_pwl): QuantizedConv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), scale=1.906625509262085, zero_point=63)\n        (bn3): BatchNormAct2d(\n          112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): Identity()\n        )\n        (drop_path): Identity()\n      )\n      (1): InvertedResidual(\n        (conv_pw): QuantizedConv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), scale=8.924609184265137, zero_point=56)\n        (bn1): BatchNormAct2d(\n          672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (conv_dw): QuantizedConv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), scale=0.6600613594055176, zero_point=66, padding=(2, 2), groups=672)\n        (bn2): BatchNormAct2d(\n          672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (se): SqueezeExcite(\n          (conv_reduce): QuantizedConv2d(672, 28, kernel_size=(1, 1), stride=(1, 1), scale=0.2649509608745575, zero_point=54)\n          (act1): SiLU(inplace=True)\n          (conv_expand): QuantizedConv2d(28, 672, kernel_size=(1, 1), stride=(1, 1), scale=0.6865613460540771, zero_point=70)\n          (gate): Sigmoid()\n        )\n        (conv_pwl): QuantizedConv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), scale=0.813129723072052, zero_point=64)\n        (bn3): BatchNormAct2d(\n          112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): Identity()\n        )\n        (drop_path): Identity()\n      )\n      (2): InvertedResidual(\n        (conv_pw): QuantizedConv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), scale=8.338939666748047, zero_point=61)\n        (bn1): BatchNormAct2d(\n          672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (conv_dw): QuantizedConv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), scale=0.6012331247329712, zero_point=58, padding=(2, 2), groups=672)\n        (bn2): BatchNormAct2d(\n          672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (se): SqueezeExcite(\n          (conv_reduce): QuantizedConv2d(672, 28, kernel_size=(1, 1), stride=(1, 1), scale=0.22887469828128815, zero_point=50)\n          (act1): SiLU(inplace=True)\n          (conv_expand): QuantizedConv2d(28, 672, kernel_size=(1, 1), stride=(1, 1), scale=0.6694828867912292, zero_point=78)\n          (gate): Sigmoid()\n        )\n        (conv_pwl): QuantizedConv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), scale=0.8249907493591309, zero_point=61)\n        (bn3): BatchNormAct2d(\n          112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): Identity()\n        )\n        (drop_path): Identity()\n      )\n    )\n    (5): Sequential(\n      (0): InvertedResidual(\n        (conv_pw): QuantizedConv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), scale=9.555721282958984, zero_point=61)\n        (bn1): BatchNormAct2d(\n          672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (conv_dw): Conv2dSame(672, 672, kernel_size=(5, 5), stride=(2, 2), groups=672, bias=False)\n        (bn2): BatchNormAct2d(\n          672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (se): SqueezeExcite(\n          (conv_reduce): QuantizedConv2d(672, 28, kernel_size=(1, 1), stride=(1, 1), scale=2.2129688262939453, zero_point=14)\n          (act1): SiLU(inplace=True)\n          (conv_expand): QuantizedConv2d(28, 672, kernel_size=(1, 1), stride=(1, 1), scale=1.0522133111953735, zero_point=56)\n          (gate): Sigmoid()\n        )\n        (conv_pwl): QuantizedConv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), scale=3.1806952953338623, zero_point=61)\n        (bn3): BatchNormAct2d(\n          192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): Identity()\n        )\n        (drop_path): Identity()\n      )\n      (1): InvertedResidual(\n        (conv_pw): QuantizedConv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), scale=15.162618637084961, zero_point=63)\n        (bn1): BatchNormAct2d(\n          1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (conv_dw): QuantizedConv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), scale=0.6081740260124207, zero_point=72, padding=(2, 2), groups=1152)\n        (bn2): BatchNormAct2d(\n          1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (se): SqueezeExcite(\n          (conv_reduce): QuantizedConv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1), scale=0.4470100700855255, zero_point=61)\n          (act1): SiLU(inplace=True)\n          (conv_expand): QuantizedConv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1), scale=0.8715055584907532, zero_point=69)\n          (gate): Sigmoid()\n        )\n        (conv_pwl): QuantizedConv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), scale=1.0346759557724, zero_point=60)\n        (bn3): BatchNormAct2d(\n          192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): Identity()\n        )\n        (drop_path): Identity()\n      )\n      (2): InvertedResidual(\n        (conv_pw): QuantizedConv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), scale=14.116510391235352, zero_point=69)\n        (bn1): BatchNormAct2d(\n          1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (conv_dw): QuantizedConv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), scale=0.5549591779708862, zero_point=69, padding=(2, 2), groups=1152)\n        (bn2): BatchNormAct2d(\n          1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (se): SqueezeExcite(\n          (conv_reduce): QuantizedConv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1), scale=0.56062912940979, zero_point=63)\n          (act1): SiLU(inplace=True)\n          (conv_expand): QuantizedConv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1), scale=0.856477677822113, zero_point=67)\n          (gate): Sigmoid()\n        )\n        (conv_pwl): QuantizedConv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), scale=2.121051073074341, zero_point=64)\n        (bn3): BatchNormAct2d(\n          192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): Identity()\n        )\n        (drop_path): Identity()\n      )\n      (3): InvertedResidual(\n        (conv_pw): QuantizedConv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), scale=18.40049934387207, zero_point=66)\n        (bn1): BatchNormAct2d(\n          1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (conv_dw): QuantizedConv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), scale=0.6685059070587158, zero_point=75, padding=(2, 2), groups=1152)\n        (bn2): BatchNormAct2d(\n          1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (se): SqueezeExcite(\n          (conv_reduce): QuantizedConv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1), scale=0.3985326588153839, zero_point=78)\n          (act1): SiLU(inplace=True)\n          (conv_expand): QuantizedConv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1), scale=0.41850119829177856, zero_point=67)\n          (gate): Sigmoid()\n        )\n        (conv_pwl): QuantizedConv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), scale=1.8179219961166382, zero_point=60)\n        (bn3): BatchNormAct2d(\n          192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): Identity()\n        )\n        (drop_path): Identity()\n      )\n    )\n    (6): Sequential(\n      (0): InvertedResidual(\n        (conv_pw): QuantizedConv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), scale=32.113746643066406, zero_point=67)\n        (bn1): BatchNormAct2d(\n          1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (conv_dw): QuantizedConv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), scale=0.48673635721206665, zero_point=58, padding=(1, 1), groups=1152)\n        (bn2): BatchNormAct2d(\n          1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (se): SqueezeExcite(\n          (conv_reduce): QuantizedConv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1), scale=0.4230678677558899, zero_point=52)\n          (act1): SiLU(inplace=True)\n          (conv_expand): QuantizedConv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1), scale=0.8746967315673828, zero_point=69)\n          (gate): Sigmoid()\n        )\n        (conv_pwl): QuantizedConv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), scale=2.297248125076294, zero_point=61)\n        (bn3): BatchNormAct2d(\n          320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): Identity()\n        )\n        (drop_path): Identity()\n      )\n    )\n  )\n  (conv_head): QuantizedConv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), scale=20.381771087646484, zero_point=70)\n  (bn2): BatchNormAct2d(\n    1280, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n    (drop): Identity()\n    (act): SiLU(inplace=True)\n  )\n  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n  (classifier): Sequential(\n    (0): QuantizedLinear(in_features=1280, out_features=264, scale=0.31526464223861694, zero_point=93, qscheme=torch.per_channel_affine)\n  )\n)"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.quantization.convert(model, inplace=True);"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T18:46:47.958556Z",
     "start_time": "2023-09-17T18:46:47.462057Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 0.414MB\n"
     ]
    }
   ],
   "source": [
    "get_model_size(model=model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T18:46:59.240298Z",
     "start_time": "2023-09-17T18:46:59.234897Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'quantized::conv2d.new' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::conv2d.new' is only available for these backends: [MPS, QuantizedCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMeta, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nMPS: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:39 [backend fallback]\nQuantizedCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/quantized/cpu/qconv.cpp:1555 [kernel]\nBackendSelect: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:491 [backend fallback]\nFunctionalize: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:280 [backend fallback]\nNamed: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:63 [backend fallback]\nAutogradOther: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:30 [backend fallback]\nAutogradCPU: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:34 [backend fallback]\nAutogradCUDA: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:42 [backend fallback]\nAutogradXLA: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:46 [backend fallback]\nAutogradMPS: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:54 [backend fallback]\nAutogradXPU: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:38 [backend fallback]\nAutogradHPU: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:67 [backend fallback]\nAutogradLazy: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:50 [backend fallback]\nAutogradMeta: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:58 [backend fallback]\nTracer: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/TraceTypeManual.cpp:294 [backend fallback]\nAutocastCPU: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:487 [backend fallback]\nAutocastCUDA: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:354 [backend fallback]\nFuncTorchBatched: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:815 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1073 [backend fallback]\nVmapMode: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:210 [backend fallback]\nPythonTLSSnapshot: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:152 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:487 [backend fallback]\nPythonDispatcher: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[65], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mget_metric_score\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[6], line 12\u001B[0m, in \u001B[0;36mget_metric_score\u001B[0;34m(model)\u001B[0m\n\u001B[1;32m     10\u001B[0m X_batch \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     11\u001B[0m y_batch \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m---> 12\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m res \u001B[38;5;241m=\u001B[39m res\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39msigmoid()\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[1;32m     15\u001B[0m y_batch_onehot \u001B[38;5;241m=\u001B[39m y_batch\n",
      "File \u001B[0;32m~/PycharmProjects/model_compression/venv/lib/python3.10/site-packages/timm/models/efficientnet.py:179\u001B[0m, in \u001B[0;36mEfficientNet.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    178\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m--> 179\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    180\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward_head(x)\n\u001B[1;32m    181\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m~/PycharmProjects/model_compression/venv/lib/python3.10/site-packages/timm/models/efficientnet.py:167\u001B[0m, in \u001B[0;36mEfficientNet.forward_features\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    165\u001B[0m     x \u001B[38;5;241m=\u001B[39m checkpoint_seq(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblocks, x, flatten\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    166\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 167\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mblocks\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    168\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv_head(x)\n\u001B[1;32m    169\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbn2(x)\n",
      "File \u001B[0;32m~/PycharmProjects/model_compression/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/PycharmProjects/model_compression/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/PycharmProjects/model_compression/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/PycharmProjects/model_compression/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/PycharmProjects/model_compression/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/PycharmProjects/model_compression/venv/lib/python3.10/site-packages/timm/models/_efficientnet_blocks.py:123\u001B[0m, in \u001B[0;36mDepthwiseSeparableConv.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    121\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m    122\u001B[0m     shortcut \u001B[38;5;241m=\u001B[39m x\n\u001B[0;32m--> 123\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv_dw\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    124\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbn1(x)\n\u001B[1;32m    125\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mse(x)\n",
      "File \u001B[0;32m~/PycharmProjects/model_compression/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/PycharmProjects/model_compression/venv/lib/python3.10/site-packages/torch/ao/nn/quantized/modules/conv.py:469\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    466\u001B[0m     _reversed_padding_repeated_twice \u001B[38;5;241m=\u001B[39m _reverse_repeat_padding(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding)\n\u001B[1;32m    467\u001B[0m     \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, _reversed_padding_repeated_twice,\n\u001B[1;32m    468\u001B[0m                   mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode)\n\u001B[0;32m--> 469\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquantized\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    470\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_packed_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscale\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mzero_point\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/model_compression/venv/lib/python3.10/site-packages/torch/_ops.py:502\u001B[0m, in \u001B[0;36mOpOverloadPacket.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    497\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    498\u001B[0m     \u001B[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001B[39;00m\n\u001B[1;32m    499\u001B[0m     \u001B[38;5;66;03m# is still callable from JIT\u001B[39;00m\n\u001B[1;32m    500\u001B[0m     \u001B[38;5;66;03m# We save the function ptr as the `op` attribute on\u001B[39;00m\n\u001B[1;32m    501\u001B[0m     \u001B[38;5;66;03m# OpOverloadPacket to access it here.\u001B[39;00m\n\u001B[0;32m--> 502\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_op\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mNotImplementedError\u001B[0m: Could not run 'quantized::conv2d.new' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::conv2d.new' is only available for these backends: [MPS, QuantizedCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMeta, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nMPS: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:39 [backend fallback]\nQuantizedCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/quantized/cpu/qconv.cpp:1555 [kernel]\nBackendSelect: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:491 [backend fallback]\nFunctionalize: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:280 [backend fallback]\nNamed: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:63 [backend fallback]\nAutogradOther: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:30 [backend fallback]\nAutogradCPU: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:34 [backend fallback]\nAutogradCUDA: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:42 [backend fallback]\nAutogradXLA: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:46 [backend fallback]\nAutogradMPS: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:54 [backend fallback]\nAutogradXPU: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:38 [backend fallback]\nAutogradHPU: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:67 [backend fallback]\nAutogradLazy: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:50 [backend fallback]\nAutogradMeta: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:58 [backend fallback]\nTracer: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/TraceTypeManual.cpp:294 [backend fallback]\nAutocastCPU: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:487 [backend fallback]\nAutocastCUDA: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:354 [backend fallback]\nFuncTorchBatched: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:815 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1073 [backend fallback]\nVmapMode: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:210 [backend fallback]\nPythonTLSSnapshot: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:152 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:487 [backend fallback]\nPythonDispatcher: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "get_metric_score(model=model);"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T18:47:02.595374Z",
     "start_time": "2023-09-17T18:47:01.879941Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
