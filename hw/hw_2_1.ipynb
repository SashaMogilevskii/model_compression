{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../scr/utils\")\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import time\n",
    "import datetime\n",
    "import warnings\n",
    "import platform\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import timm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from create_dataset import BirdDataset\n",
    "from base_utils import set_seed\n",
    "from metrics import validation_epoch_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Информация о процессоре: Intel64 Family 6 Model 158 Stepping 10, GenuineIntel\n",
      "Доступно GPU: 1\n",
      "GPU 1: NVIDIA GeForce GTX 1070 Ti\n"
     ]
    }
   ],
   "source": [
    "# Информация о железе, на котором тестируется модель\n",
    "\n",
    "processor_info = platform.processor()\n",
    "print(\"Информация о процессоре:\", processor_info)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(\"Доступно GPU:\", num_gpus)\n",
    "    for i in range(num_gpus):\n",
    "        gpu = torch.cuda.get_device_name(i)\n",
    "        print(f\"GPU {i + 1}: {gpu}\")   \n",
    "else:\n",
    "    print(\"GPU недоступны на данной системе.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/data.csv\")\n",
    "df_test = df[df.fold == 3].reset_index(drop=True)  \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_base = torch.load(\"../experiment/14_September_2023_16_29/model_tf_efficientnet_b0_last_version.pt\").to(device)\n",
    "dataset_test = BirdDataset(df=df_test,\n",
    "                           path_to_folder_with_audio=\"../data\")\n",
    "valid_loader = DataLoader(dataset_test,\n",
    "                          batch_size=4)\n",
    "model_base.eval()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Базовая модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 16.738MB\n"
     ]
    }
   ],
   "source": [
    "param_size = 0\n",
    "for param in model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "for buffer in model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "print('model size: {:.3f}MB'.format(size_all_mb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим работу модели, при переводе модели из fp32 -> fp16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 8.369MB\n"
     ]
    }
   ],
   "source": [
    "param_size = 0\n",
    "for param in model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "for buffer in model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "print('model size: {:.3f}MB'.format(size_all_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 771/771 [03:31<00:00,  3.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# Проверим точность модели при запуске на нашем валидационном датасете\n",
    "predicted_labels_list = None\n",
    "true_labels_list = None\n",
    "metric = validation_epoch_end\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(valid_loader):\n",
    "        X_batch = batch[0].to(torch.float16).to(device)\n",
    "        y_batch = batch[1].to(device)\n",
    "        res = model.forward(X_batch)\n",
    "\n",
    "        res = res.detach().sigmoid().cpu().numpy()\n",
    "        y_batch_onehot = y_batch\n",
    "        y_batch_onehot = y_batch_onehot.unsqueeze(1).detach().cpu().numpy()\n",
    "        y_batch_onehot = y_batch_onehot.squeeze()\n",
    "\n",
    "        if predicted_labels_list is None:\n",
    "            predicted_labels_list = res\n",
    "            true_labels_list = y_batch_onehot\n",
    "        else:\n",
    "            predicted_labels_list = np.concatenate([predicted_labels_list, res], axis=0)\n",
    "            true_labels_list = np.concatenate([true_labels_list, y_batch_onehot], axis=0)      \n",
    "\n",
    "        del batch, res\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predicted_labels = np.vstack(predicted_labels_list)\n",
    "all_true_labels = np.vstack(true_labels_list)\n",
    "all_true_labels = np.squeeze(all_true_labels)\n",
    "mask = (all_true_labels > 0) & (all_true_labels < 1)\n",
    "all_true_labels[mask] = 0\n",
    "avg_metric = metric(all_true_labels, all_predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наши метрики на model.half()\n",
      "Время работы модели на всем батче 211.396 сек.\n",
      "Время работы модели на одном сэмпле (AVG) 0.069 сек.\n",
      "Метрики качества\n",
      "metric val_RMAP : 0.035317\n",
      "metric CMAP_5 : 0.0204819\n"
     ]
    }
   ],
   "source": [
    "# Мы понимаем, что ко времени работы модели добавляем время обработки батчей и добавления аугментаций к стартовым данным\n",
    "\n",
    "print(\"Наши метрики на model.half()\")\n",
    "print(f\"Время работы модели на всем батче {t:.<2g} сек.\")\n",
    "print(f\"Время работы модели на одном сэмпле (AVG) {round(t/ len(dataset_test), 3) } сек.\")\n",
    "print(\"Метрики качества\")\n",
    "for m in avg_metric:\n",
    "    print(f\"metric {m} : {avg_metric[m]:.<5g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Вес модели уменьшился, но метрики просели очень сильно. Данный способ компрессии модели нам не подходит. См. блокнот hw_2_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
