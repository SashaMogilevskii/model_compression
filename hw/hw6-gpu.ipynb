{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install optimum[onnxruntime-gpu] ","metadata":{"execution":{"iopub.status.busy":"2023-10-27T17:00:33.982863Z","iopub.execute_input":"2023-10-27T17:00:33.983150Z","iopub.status.idle":"2023-10-27T17:01:07.860185Z","shell.execute_reply.started":"2023-10-27T17:00:33.983125Z","shell.execute_reply":"2023-10-27T17:01:07.859214Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting optimum[onnxruntime-gpu]\n  Downloading optimum-1.13.2.tar.gz (300 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.0/301.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting coloredlogs (from optimum[onnxruntime-gpu])\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from optimum[onnxruntime-gpu]) (1.12)\nRequirement already satisfied: transformers[sentencepiece]>=4.26.0 in /opt/conda/lib/python3.10/site-packages (from optimum[onnxruntime-gpu]) (4.33.0)\nRequirement already satisfied: torch>=1.9 in /opt/conda/lib/python3.10/site-packages (from optimum[onnxruntime-gpu]) (2.0.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from optimum[onnxruntime-gpu]) (21.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from optimum[onnxruntime-gpu]) (1.23.5)\nRequirement already satisfied: huggingface-hub>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from optimum[onnxruntime-gpu]) (0.16.4)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from optimum[onnxruntime-gpu]) (2.1.0)\nRequirement already satisfied: onnx in /opt/conda/lib/python3.10/site-packages (from optimum[onnxruntime-gpu]) (1.14.1)\nCollecting onnxruntime-gpu>=1.11.0 (from optimum[onnxruntime-gpu])\n  Downloading onnxruntime_gpu-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (153.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.4/153.4 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting evaluate (from optimum[onnxruntime-gpu])\n  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: protobuf>=3.20.1 in /opt/conda/lib/python3.10/site-packages (from optimum[onnxruntime-gpu]) (3.20.3)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from optimum[onnxruntime-gpu]) (0.22.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum[onnxruntime-gpu]) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets->optimum[onnxruntime-gpu]) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->optimum[onnxruntime-gpu]) (2.0.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum[onnxruntime-gpu]) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum[onnxruntime-gpu]) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->optimum[onnxruntime-gpu]) (3.3.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->optimum[onnxruntime-gpu]) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum[onnxruntime-gpu]) (2023.9.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->optimum[onnxruntime-gpu]) (3.8.4)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum[onnxruntime-gpu]) (0.18.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum[onnxruntime-gpu]) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum[onnxruntime-gpu]) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum[onnxruntime-gpu]) (4.6.3)\nRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime-gpu>=1.11.0->optimum[onnxruntime-gpu]) (23.5.26)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->optimum[onnxruntime-gpu]) (3.0.9)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->optimum[onnxruntime-gpu]) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->optimum[onnxruntime-gpu]) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]>=4.26.0->optimum[onnxruntime-gpu]) (2023.6.3)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]>=4.26.0->optimum[onnxruntime-gpu]) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]>=4.26.0->optimum[onnxruntime-gpu]) (0.3.3)\nRequirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]>=4.26.0->optimum[onnxruntime-gpu]) (0.1.99)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->optimum[onnxruntime-gpu]) (5.9.3)\nCollecting humanfriendly>=9.1 (from coloredlogs->optimum[onnxruntime-gpu])\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->optimum[onnxruntime-gpu]) (1.3.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum[onnxruntime-gpu]) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum[onnxruntime-gpu]) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum[onnxruntime-gpu]) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum[onnxruntime-gpu]) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum[onnxruntime-gpu]) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum[onnxruntime-gpu]) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum[onnxruntime-gpu]) (1.3.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets->optimum[onnxruntime-gpu]) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets->optimum[onnxruntime-gpu]) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets->optimum[onnxruntime-gpu]) (2023.7.22)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.9->optimum[onnxruntime-gpu]) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum[onnxruntime-gpu]) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum[onnxruntime-gpu]) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum[onnxruntime-gpu]) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum[onnxruntime-gpu]) (1.16.0)\nBuilding wheels for collected packages: optimum\n  Building wheel for optimum (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for optimum: filename=optimum-1.13.2-py3-none-any.whl size=395599 sha256=ea9bf36079e3ba640034fe6630ea4e5e7b17a2ca84ae7f4cc903442f833a03d2\n  Stored in directory: /root/.cache/pip/wheels/6e/b7/2c/79405d98f0943373d8546daeae25a3d377f7659ca0cbe48699\nSuccessfully built optimum\nInstalling collected packages: humanfriendly, coloredlogs, onnxruntime-gpu, optimum, evaluate\nSuccessfully installed coloredlogs-15.0.1 evaluate-0.4.1 humanfriendly-10.0 onnxruntime-gpu-1.16.1 optimum-1.13.2\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install transformers --upgrade","metadata":{"execution":{"iopub.status.busy":"2023-10-27T17:01:07.861983Z","iopub.execute_input":"2023-10-27T17:01:07.862289Z","iopub.status.idle":"2023-10-27T17:01:30.046958Z","shell.execute_reply.started":"2023-10-27T17:01:07.862262Z","shell.execute_reply":"2023-10-27T17:01:30.045864Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.33.0)\nCollecting transformers\n  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nCollecting tokenizers<0.15,>=0.14 (from transformers)\n  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\nInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.13.3\n    Uninstalling tokenizers-0.13.3:\n      Successfully uninstalled tokenizers-0.13.3\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.33.0\n    Uninstalling transformers-4.33.0:\n      Successfully uninstalled transformers-4.33.0\nSuccessfully installed tokenizers-0.14.1 transformers-4.34.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport os\nimport numpy as np\nfrom torch import nn\nimport torch.nn.utils.prune as prune\nimport pandas as pd\nimport transformers\nfrom transformers import pipeline\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom datasets import load_dataset\nfrom evaluate import evaluator\nimport evaluate\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom optimum.pipelines import pipeline\nfrom optimum.onnxruntime import ORTOptimizer, ORTModelForSequenceClassification\nfrom optimum.onnxruntime.configuration import OptimizationConfig\n\nfrom tqdm import tqdm\n\nimport time","metadata":{"execution":{"iopub.status.busy":"2023-10-27T17:01:30.048653Z","iopub.execute_input":"2023-10-27T17:01:30.049529Z","iopub.status.idle":"2023-10-27T17:01:48.251118Z","shell.execute_reply.started":"2023-10-27T17:01:30.049490Z","shell.execute_reply":"2023-10-27T17:01:48.250258Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"class Config:\n    device = \"cuda\"\n    model_checkpoint = \"unitary/toxic-bert\"\n    save_folder = 'onnx_checkpoint'\n    b_s = 16","metadata":{"execution":{"iopub.status.busy":"2023-10-27T17:01:48.253114Z","iopub.execute_input":"2023-10-27T17:01:48.253732Z","iopub.status.idle":"2023-10-27T17:01:48.259925Z","shell.execute_reply.started":"2023-10-27T17:01:48.253703Z","shell.execute_reply":"2023-10-27T17:01:48.258969Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(Config.model_checkpoint).to(Config.device)\ntokenizer = transformers.BertTokenizer.from_pretrained('bert-base-cased')","metadata":{"execution":{"iopub.status.busy":"2023-10-27T17:01:48.261111Z","iopub.execute_input":"2023-10-27T17:01:48.261434Z","iopub.status.idle":"2023-10-27T17:01:58.150013Z","shell.execute_reply.started":"2023-10-27T17:01:48.261403Z","shell.execute_reply":"2023-10-27T17:01:58.148985Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8906766d766045fba785bb8b1646f001"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c91a4088dad24f1aa0fdee87bf625fe6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"712e695d58a14e909c5034d34c5e08d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b589a377c1224faa8aba38e0bb704e24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93eb12812faa4f4882aa5c7a1d24327d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35e0e301c997453384b639ca4ed379fe"}},"metadata":{}}]},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip\")","metadata":{"execution":{"iopub.status.busy":"2023-10-27T17:01:58.151284Z","iopub.execute_input":"2023-10-27T17:01:58.151541Z","iopub.status.idle":"2023-10-27T17:01:59.933825Z","shell.execute_reply.started":"2023-10-27T17:01:58.151518Z","shell.execute_reply":"2023-10-27T17:01:59.933003Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"data_1 = data[data.toxic == 0].sample(200)\ndata_2 = data[data.severe_toxic == 1].sample(200)\ndata_3 = data[data.identity_hate == 1].sample(200)\ndata_4 = data[data.obscene == 1].sample(200)\ndata_5 = data[data.threat == 1].sample(200)\ndata_6 = data[data.insult == 1].sample(200)\ndata_7 = data[data.identity_hate == 1].sample(200)\ndata_8 = data.sample(1000)\nnew_data = pd.concat([data_1,\n                      data_2, \n                      data_3,\n                      data_4,\n                      data_5,\n                      data_6,\n                      data_7,\n                      data_8,\n                     ]).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T17:01:59.934978Z","iopub.execute_input":"2023-10-27T17:01:59.935264Z","iopub.status.idle":"2023-10-27T17:01:59.986395Z","shell.execute_reply.started":"2023-10-27T17:01:59.935239Z","shell.execute_reply":"2023-10-27T17:01:59.985580Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class ToxicDs(Dataset):\n    def __init__(self, data, tokenizer, length):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, ind):\n        row = self.data.iloc[ind]\n        sentence = row['comment_text']\n        bert_sentence = self.tokenizer(sentence,\n                        max_length=self.max_length,\n                        pad_to_max_length=True,\n                        add_special_tokens=True)\n\n        return {\n            \"id\": torch.LongTensor(bert_sentence['input_ids']),\n            \"mask\":  torch.LongTensor(bert_sentence['attention_mask']),                      \n        }\n    \nds = ToxicDs(new_data, length = 256, tokenizer=tokenizer)\ntoxic_dataloader = DataLoader(ds, batch_size = Config.b_s, shuffle= False, drop_last=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T17:01:59.987458Z","iopub.execute_input":"2023-10-27T17:01:59.987771Z","iopub.status.idle":"2023-10-27T17:02:00.154650Z","shell.execute_reply.started":"2023-10-27T17:01:59.987728Z","shell.execute_reply":"2023-10-27T17:02:00.153453Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"model.eval()\n\nstart_time = time.time()\nlst_times = []\nwith torch.no_grad():\n    for batch in tqdm(toxic_dataloader):\n        st_b_time = time.time()\n        ids = batch[\"id\"].to(Config.device)\n        masks = batch[\"mask\"].to(Config.device)\n        y_true = model(ids, masks)['logits']\n        lst_times.append(time.time() - st_b_time)\nfinish_time = time.time() - start_time","metadata":{"execution":{"iopub.status.busy":"2023-10-27T17:02:00.155893Z","iopub.execute_input":"2023-10-27T17:02:00.156174Z","iopub.status.idle":"2023-10-27T17:02:19.689992Z","shell.execute_reply.started":"2023-10-27T17:02:00.156150Z","shell.execute_reply":"2023-10-27T17:02:19.689061Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"  0%|          | 0/150 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n  1%|          | 1/150 [00:01<04:54,  1.98s/it]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|██████████| 150/150 [00:19<00:00,  7.69it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f\"Средняя время работы на батче: {np.round(np.mean(lst_times), 3)} сек\")\nprint(f\"Общее время работы: {np.round(np.mean(finish_time), 3)} сек\")","metadata":{"execution":{"iopub.status.busy":"2023-10-27T17:02:19.693052Z","iopub.execute_input":"2023-10-27T17:02:19.693352Z","iopub.status.idle":"2023-10-27T17:02:19.699049Z","shell.execute_reply.started":"2023-10-27T17:02:19.693327Z","shell.execute_reply":"2023-10-27T17:02:19.697807Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Средняя время работы на батче: 0.09 сек\nОбщее время работы: 19.518 сек\n","output_type":"stream"}]},{"cell_type":"code","source":"ort_model = ORTModelForSequenceClassification.from_pretrained(Config.model_checkpoint, export=True,)\ntokenizer_ort = AutoTokenizer.from_pretrained(Config.model_checkpoint)\n# ort_model.save_pretrained(Config.save_folder)\n# tokenizer.save_pretrained(Config.save_folder)\n\nonnx_classifier = pipeline(\"text-classification\",model=ort_model,tokenizer=tokenizer_ort, device = Config.device)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T17:02:19.700300Z","iopub.execute_input":"2023-10-27T17:02:19.700594Z","iopub.status.idle":"2023-10-27T17:02:33.645562Z","shell.execute_reply.started":"2023-10-27T17:02:19.700549Z","shell.execute_reply":"2023-10-27T17:02:33.644671Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Framework not specified. Using pt to export to ONNX.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/174 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28b25d1fc6f34bb3bd36b455754c202a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b54b117a32aa4dd48da390059f92527b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d640c1d2fcb845ba9e0c4bb7d806203e"}},"metadata":{}},{"name":"stderr","text":"Using the export variant default. Available variants are:\n\t- default: The default ONNX variant.\nUsing framework PyTorch: 2.0.0\nOverriding 1 configuration item(s)\n\t- use_cache -> False\n","output_type":"stream"},{"name":"stdout","text":"================ Diagnostic Run torch.onnx.export version 2.0.0 ================\nverbose: False, log level: Level.ERROR\n======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n\n","output_type":"stream"},{"name":"stderr","text":"use_io_binding was set to False, setting it to True because it can provide a huge speedup on GPUs. It is possible to disable this feature manually by setting the use_io_binding attribute back to False.\n2023-10-27 17:02:33.044138920 [W:onnxruntime:, session_state.cc:1162 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\n2023-10-27 17:02:33.044184284 [W:onnxruntime:, session_state.cc:1164 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\n","output_type":"stream"}]},{"cell_type":"code","source":"ds_onnx = ToxicDs(new_data, length = 128, tokenizer=tokenizer_ort)\ntoxic_dataloader_onnx = DataLoader(ds_onnx, batch_size = Config.b_s, shuffle= False, drop_last=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T17:03:38.969956Z","iopub.execute_input":"2023-10-27T17:03:38.970320Z","iopub.status.idle":"2023-10-27T17:03:38.975313Z","shell.execute_reply.started":"2023-10-27T17:03:38.970284Z","shell.execute_reply":"2023-10-27T17:03:38.974234Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"start_time = time.time()\nlst_times = []\n\nfor batch in tqdm(toxic_dataloader_onnx):\n    st_b_time = time.time()\n    ids = batch[\"id\"].to(Config.device)\n    masks = batch[\"mask\"].to(Config.device)\n\n    y_true = onnx_classifier.forward({\n        \"input_ids\":ids,\n        \"attention_mask\":masks,\n        'token_type_ids': torch.ones_like(masks)\n    })\n    lst_times.append(time.time() - st_b_time)\nfinish_time = time.time() - start_time","metadata":{"execution":{"iopub.status.busy":"2023-10-27T17:03:39.378640Z","iopub.execute_input":"2023-10-27T17:03:39.379673Z","iopub.status.idle":"2023-10-27T17:03:49.894660Z","shell.execute_reply.started":"2023-10-27T17:03:39.379632Z","shell.execute_reply":"2023-10-27T17:03:49.893723Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"100%|██████████| 150/150 [00:10<00:00, 14.28it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f\"Средняя время работы на батче: {np.round(np.mean(lst_times), 3)} сек\")\nprint(f\"Общее время работы: {np.round(np.mean(finish_time), 3)} сек\")\n","metadata":{"execution":{"iopub.status.busy":"2023-10-27T17:03:49.896671Z","iopub.execute_input":"2023-10-27T17:03:49.897426Z","iopub.status.idle":"2023-10-27T17:03:49.902239Z","shell.execute_reply.started":"2023-10-27T17:03:49.897389Z","shell.execute_reply":"2023-10-27T17:03:49.901347Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Средняя время работы на батче: 0.058 сек\nОбщее время работы: 10.508 сек\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f\"Ускорение общего времени работы: {((19.518 - 10.508 ) / 19.518 * 100):.2f}%\")\nprint(f\"Ускорение работы на батче : {((0.09 - 0.058 ) / 0.09 * 100):.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2023-10-27T17:04:44.908639Z","iopub.execute_input":"2023-10-27T17:04:44.909617Z","iopub.status.idle":"2023-10-27T17:04:44.914991Z","shell.execute_reply.started":"2023-10-27T17:04:44.909554Z","shell.execute_reply":"2023-10-27T17:04:44.913831Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Ускорение общего времени работы: 46.16%\nУскорение работы на батче : 35.56%\n","output_type":"stream"}]},{"cell_type":"code","source":"# Базовая модель\nmodel_size_bytes = sum(p.numel() for p in model.parameters() if p.requires_grad) * 4\n\nmodel_size_mb = model_size_bytes / (1024 * 1024)\nprint(f\"Размер модели: {model_size_mb:.2f} MB\")","metadata":{"execution":{"iopub.status.busy":"2023-10-27T17:02:53.510550Z","iopub.execute_input":"2023-10-27T17:02:53.510937Z","iopub.status.idle":"2023-10-27T17:02:53.522962Z","shell.execute_reply.started":"2023-10-27T17:02:53.510911Z","shell.execute_reply":"2023-10-27T17:02:53.522098Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Размер модели: 417.66 MB\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# ONNX модель\ntemp_model_path = \"temp_ort_model\"\n\nort_model.save_pretrained(temp_model_path)\nmodel_size = os.path.getsize(temp_model_path)\nmodel_size_mb = model_size_bytes / (1024 * 1024)\nprint(f\"Размер модели: {(model_size / 10):.2f} MB\")","metadata":{"execution":{"iopub.status.busy":"2023-10-27T17:02:53.524284Z","iopub.execute_input":"2023-10-27T17:02:53.524994Z","iopub.status.idle":"2023-10-27T17:02:54.241026Z","shell.execute_reply.started":"2023-10-27T17:02:53.524959Z","shell.execute_reply":"2023-10-27T17:02:54.240231Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Размер модели: 409.60 MB\n","output_type":"stream"}]}]}