{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow-model-optimization pyyaml loguru model-profiler\n",
        "!pip install timm\n",
        "!pip install onnxruntime\n",
        "!pip install onnx"
      ],
      "metadata": {
        "id": "UdovRxkWT-bu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7754033b-c719-431a-e53d-3927e2a73776"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/241.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/241.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/62.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone -b dev https://github.com/SashaMogilevskii/model_compression"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qqe1OBgUMv-B",
        "outputId": "969243c3-6c83-4242-dc7a-56f47c06b837"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'model_compression'...\n",
            "remote: Enumerating objects: 127, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 127 (delta 0), reused 8 (delta 0), pack-reused 118\u001b[K\n",
            "Receiving objects: 100% (127/127), 62.94 MiB | 26.78 MiB/s, done.\n",
            "Resolving deltas: 100% (49/49), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd model_compression/data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RruEaGq5MwQe",
        "outputId": "d0717d25-fcd1-4de7-8cf8-60293f872ae8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/model_compression/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 18gCWw8IkqbFr9X0TCBM5LCpsU11Ga6tc\n",
        "!unrar x -y data.rar"
      ],
      "metadata": {
        "id": "bLtmMiSwNR9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../hw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OjzoAtiNdM9",
        "outputId": "f580e509-36cd-4600-be76-40e4257de608"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/model_compression/hw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"../\")"
      ],
      "metadata": {
        "id": "58DHnpFRUv3O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "44e5d037-859c-44d6-fbc0-93e2aeba0cd1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-678de0ced9f0>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtimm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'timm'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "zmE-9Q4UMq63"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"../scr/utils\")\n",
        "import os\n",
        "\n",
        "from copy import deepcopy\n",
        "import time\n",
        "import platform\n",
        "import onnx\n",
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import timm\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.utils.prune as prune\n",
        "\n",
        "from scr.utils.create_dataset import BirdDataset\n",
        "from scr.utils.metrics import validation_epoch_end"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Информация о железе, на котором тестируется модель\n",
        "\n",
        "processor_info = platform.processor()\n",
        "print(\"Информация о процессоре:\", processor_info)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    num_gpus = torch.cuda.device_count()\n",
        "    print(\"Доступно GPU:\", num_gpus)\n",
        "    for i in range(num_gpus):\n",
        "        gpu = torch.cuda.get_device_name(i)\n",
        "        print(f\"GPU {i + 1}: {gpu}\")\n",
        "else:\n",
        "    print(\"GPU недоступны на данной системе.\")"
      ],
      "metadata": {
        "id": "FkK_B108TD5z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deaf82e3-8d17-4b90-d175-0763833b7a7d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Информация о процессоре: x86_64\n",
            "GPU недоступны на данной системе.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_metric_score(model):\n",
        "    predicted_labels_list = None\n",
        "    true_labels_list = None\n",
        "    metric = validation_epoch_end\n",
        "    start_time = time.time()\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(valid_loader):\n",
        "            X_batch = batch[0].to(device)\n",
        "            y_batch = batch[1].to(device)\n",
        "            res = model.forward(X_batch)\n",
        "\n",
        "            res = res.detach().sigmoid().cpu().numpy()\n",
        "            y_batch_onehot = y_batch\n",
        "            y_batch_onehot = y_batch_onehot.unsqueeze(1).detach().cpu().numpy()\n",
        "            y_batch_onehot = y_batch_onehot.squeeze()\n",
        "\n",
        "            if predicted_labels_list is None:\n",
        "                predicted_labels_list = res\n",
        "                true_labels_list = y_batch_onehot\n",
        "            else:\n",
        "                predicted_labels_list = np.concatenate([predicted_labels_list, res], axis=0)\n",
        "                true_labels_list = np.concatenate([true_labels_list, y_batch_onehot], axis=0)\n",
        "\n",
        "            del batch, res\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "\n",
        "    all_predicted_labels = np.vstack(predicted_labels_list)\n",
        "    all_true_labels = np.vstack(true_labels_list)\n",
        "    all_true_labels = np.squeeze(all_true_labels)\n",
        "    mask = (all_true_labels > 0) & (all_true_labels < 1)\n",
        "    all_true_labels[mask] = 0\n",
        "    avg_metric = metric(all_true_labels.reshape(100, -1), all_predicted_labels.reshape(100, -1))\n",
        "    t = end_time - start_time\n",
        "\n",
        "    # Мы понимаем, что ко времени работы модели добавляем время обработки батчей и добавления аугментаций к стартовым данным\n",
        "\n",
        "    print(\"Наши метрики на нашей базовой модели:\")\n",
        "    print(f\"Время работы модели на всем батче {t:.<2g} сек.\")\n",
        "    print(f\"Время работы модели на одном сэмпле (AVG) {round(t/ len(dataset_test), 3) } сек.\")\n",
        "    print(\"Метрики качества:\")\n",
        "    for m in avg_metric:\n",
        "        print(f\"metric {m} : {avg_metric[m]:.<5g}\")\n",
        "\n",
        "import math\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + math.exp(-x))\n",
        "\n",
        "sigmoid_v = np.vectorize(sigmoid)\n",
        "\n",
        "def get_metric_score_onnx(ort_session, loader, validation_epoch_end):\n",
        "    predicted_labels_list = None\n",
        "    true_labels_list = None\n",
        "    start_time = time.time()\n",
        "\n",
        "    for batch in tqdm(loader):\n",
        "        X_batch = batch[0].numpy()\n",
        "        y_batch = batch[1]\n",
        "\n",
        "        # ort_inputs = {'x.1':  np.expand_dims(X_batch, 0)}\n",
        "        ort_inputs = {'x.1':  X_batch}\n",
        "        ort_outputs = ort_session.run(None, ort_inputs)\n",
        "\n",
        "        res = sigmoid_v(ort_outputs[0]).reshape(-1, 1)\n",
        "\n",
        "        y_batch_onehot = y_batch\n",
        "        y_batch_onehot = y_batch_onehot.reshape(-1, 1)\n",
        "\n",
        "        if predicted_labels_list is None:\n",
        "            predicted_labels_list = res\n",
        "            true_labels_list = y_batch_onehot\n",
        "        else:\n",
        "            predicted_labels_list = np.concatenate([predicted_labels_list, res], axis=0)\n",
        "            true_labels_list = np.concatenate([true_labels_list, y_batch_onehot], axis=0)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    all_predicted_labels = np.vstack(predicted_labels_list)\n",
        "    all_true_labels = np.vstack(true_labels_list)\n",
        "    all_true_labels = np.squeeze(all_true_labels)\n",
        "    mask = (all_true_labels > 0) & (all_true_labels < 1)\n",
        "    all_true_labels[mask] = 0\n",
        "\n",
        "    avg_metric = validation_epoch_end(all_true_labels.reshape(100, -1), all_predicted_labels.reshape(100, -1))\n",
        "    t = end_time - start_time\n",
        "\n",
        "    print(\"Наши метрики на нашей ONNX модели:\")\n",
        "    print(f\"Время работы модели на всем батче {t:.2f} сек.\")\n",
        "    print(f\"Время работы модели на одном сэмпле (AVG) {round(t / len(valid_loader), 3)} сек.\")\n",
        "    print(\"Метрики качества:\")\n",
        "    for m in avg_metric:\n",
        "        print(f\"metric {m} : {avg_metric[m]:.5f}\")\n"
      ],
      "metadata": {
        "id": "MyO82MSwSjjU"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_size(model):\n",
        "    param_size = 0\n",
        "    for param in model.parameters():\n",
        "        param_size += param.nelement() * param.element_size()\n",
        "    buffer_size = 0\n",
        "    for buffer in model.buffers():\n",
        "        buffer_size += buffer.nelement() * buffer.element_size()\n",
        "\n",
        "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
        "    print('model size: {:.3f}MB'.format(size_all_mb))\n",
        "\n",
        "\n",
        "def get_onnx_model_size(onnx_model_path):\n",
        "    # Получите размер файла ONNX модели\n",
        "    model_size_bytes = os.path.getsize(onnx_model_path)\n",
        "\n",
        "    # Переведите размер в мегабайты (MB)\n",
        "    model_size_mb = model_size_bytes / (1024 ** 2)\n",
        "\n",
        "    print(f'ONNX модель размером: {model_size_mb:.3f} MB')\n",
        "\n",
        "def count_zero_weights(model):\n",
        "    num_zero_weights = 0\n",
        "    total_weights = 0\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            num_zero_weights += torch.sum(param == 0).item()\n",
        "            total_weights += param.numel()\n",
        "\n",
        "    sparsity = num_zero_weights / total_weights\n",
        "    print(f\"Разреженность весов: {sparsity * 100:.2f}%\")\n",
        "    return sparsity"
      ],
      "metadata": {
        "id": "6MR91puiVoBW"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"../data/data.csv\")\n",
        "df_test = df[df.fold == 3].sample(n=100, random_state=42).reset_index(drop=True)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_base = torch.load(\"../models/model_pytorch/model_tf_efficientnet_b0_last_version.pt\", map_location=device).to(device)\n",
        "dataset_test = BirdDataset(df=df_test, path_to_folder_with_audio=\"../data/data/\")\n",
        "valid_loader = DataLoader(dataset_test, batch_size=1)\n",
        "model_base.eval()\n",
        "device"
      ],
      "metadata": {
        "id": "Q0a7nzewPBzF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfa977c7-b67b-493e-941a-ae22e3e0a8db"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_metric_score(model_base)"
      ],
      "metadata": {
        "id": "d-FY3t4M0r_n",
        "outputId": "b3f92317-169d-4756-cb8d-12736b60a939",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:35<00:00,  2.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Наши метрики на нашей базовой модели:\n",
            "Время работы модели на всем батче 35.5127 сек.\n",
            "Время работы модели на одном сэмпле (AVG) 0.355 сек.\n",
            "Метрики качества:\n",
            "metric val_RMAP : 0.702789\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1\n",
        "x = torch.randn(batch_size, 3, 626, 256, requires_grad=True)\n",
        "torch_out = model_base(x)\n",
        "\n",
        "torch.onnx.export(model_base, x, \"../models/model.onnx\", opset_version=11)"
      ],
      "metadata": {
        "id": "191aoAddb6Qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_model = onnx.load(\"../models/model.onnx\")\n",
        "onnx.checker.check_model(onnx_model)"
      ],
      "metadata": {
        "id": "cH5L2YVbb6bU"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ort_sess = ort.InferenceSession(\"../models/model.onnx\")\n",
        "\n",
        "input = next(iter(dataset_test))[0]\n",
        "outputs = ort_sess.run(None, {'x.1': np.expand_dims(input.numpy(), 0)})"
      ],
      "metadata": {
        "id": "_Q_n_cBhmI11"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_metric_score_onnx(ort_sess, valid_loader_onnx, validation_epoch_end)"
      ],
      "metadata": {
        "id": "_UbqM1CJmOy8",
        "outputId": "8ab12c3a-47f7-41ff-edb6-a7e3bdf5938f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 154,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:22<00:00,  4.49it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(26400,) (26400, 1)\n",
            "Наши метрики на нашей ONNX модели:\n",
            "Время работы модели на всем батче 22.29 сек.\n",
            "Время работы модели на одном сэмпле (AVG) 0.223 сек.\n",
            "Метрики качества:\n",
            "metric val_RMAP : 0.71490\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_onnx_model_size(\"../models/model.onnx\")"
      ],
      "metadata": {
        "id": "Vd4MvapnmO4e",
        "outputId": "8938de0d-421c-4209-edea-af405c71c891",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ONNX модель размером: 16.617 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U2TkjkYqmO6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Du0IMremO9Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}