2023-09-14 16:29:48| file for running: E:\Project_python\model_compression\scr\train.py
2023-09-14 16:29:48| import os
import yaml
import time
import datetime
import warnings
from tqdm import tqdm

import torch
import timm
import pandas as pd
import numpy as np
import torch.nn as nn

from box import Box
from torch.utils.data import DataLoader
from loguru import logger
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR

from utils.create_dataset import BirdDataset
from utils.base_utils import set_seed
from utils.metrics import validation_epoch_end

warnings.filterwarnings("ignore", category=UserWarning)
date_now = datetime.datetime.now().strftime("%d_%B_%Y_%H_%M")

path_save = os.path.join("../experiment", date_now)
if not os.path.exists(path_save):
    os.makedirs(path_save)

# Load config with params for training
with open("config.yaml", "r") as f:
    config = yaml.load(f, Loader=yaml.SafeLoader)
    config = Box(config)

logger.add(f"{path_save}/info_log{date_now}.log",
           format="<red>{time:YYYY-MM-DD HH:mm:ss}</red>| {message}")

file_name = __file__
logger.info(f'file for running: {file_name}')
with open(file_name, 'r') as file:
    code = file.read()
    logger.info(code)

logger.info(f"Folder with experiment- {path_save}")
logger.info("----------params----------")

for param in config:
    logger.info(f"{param}: {str(config[param])}")

# Create device for training and set_seed:
config.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
set_seed(seed=config.seed)

# Read data
df = pd.read_csv("../data/data.csv")
df_train, df_test = (df[df.fold != 3].reset_index(drop=True),
                     df[df.fold == 3].reset_index(drop=True)
                     )

logger.info(f"Size df_train- {df_train.shape[0]}")
logger.info(f"Size df_test- {df_test.shape[0]}")

dataset_train = BirdDataset(df=df_train,
                            path_to_folder_with_audio=config.path_to_files_base
                            )
dataset_test = BirdDataset(df=df_test,
                           path_to_folder_with_audio=config.path_to_files_base
                           )

train_loader = DataLoader(dataset_train,
                          batch_size=config.batch_size,
                          shuffle=True,
                          num_workers=config.num_workers)
valid_loader = DataLoader(dataset_test,
                          batch_size=config.batch_size,
                          num_workers=config.num_workers)

logger.info(f"New experiment")
model_name = config.model_name
model = timm.create_model(model_name, pretrained=True).to(config.device)
model.classifier = nn.Sequential(
    nn.Linear(model.classifier.in_features, 264)
)
if torch.cuda.device_count() > 1:
    print("Let's use", torch.cuda.device_count(), "GPUs!")
    model = nn.DataParallel(model)
model.to(config.device)

if config.metric == 'custom':
    metric = validation_epoch_end

if config.loss_f == "nn.BCEWithLogitsLoss()":
    loss_f = nn.BCEWithLogitsLoss()

# optimizer
if config.optimizer == "Adam":
    optimizer = torch.optim.Adam(model.parameters(),
                                 lr=config.optimizer_lr,
                                 weight_decay=config.optimizer_wd
                                 )

if config.scheduler == "CosineAnnealingWarmRestarts":
    logger.info(f"Scheduler - {config.scheduler}")
    scheduler = CosineAnnealingWarmRestarts(optimizer,
                                            T_0=10,
                                            T_mult=2,
                                            eta_min=0.000001,
                                            last_epoch=-1)

# Train_loop
logger.info(f"Starting train. Model - {config.model_name}")
for epoch_i in range(1, config.epochs + 1):
    if config.debug:
        k = 1
    start = time.time()
    logger.info(f'---------------------epoch:{epoch_i}/{config.epochs}---------------------')

    # loss
    avg_train_loss = 0
    avg_val_loss = 0
    predicted_labels_list = None
    true_labels_list = None

    ############## Train #############
    model.train()
    train_pbar = tqdm(train_loader, desc="Training")
    for batch in train_pbar:
        X_batch = batch[0].to(config.device)
        y_batch = batch[1].to(config.device)

        optimizer.zero_grad()
        res = model.forward(X_batch)
        loss = loss_f(res.float(), y_batch)

        if torch.cuda.is_available():
            train_pbar.set_postfix(gpu_load=f"{torch.cuda.memory_allocated() / 1024 ** 3:.2f}GB",
                                   loss=f"{loss.item():.4f}")
        else:
            train_pbar.set_postfix(loss=f"{loss.item():.4f}")

        loss.backward()
        optimizer.step()

        avg_train_loss += loss * len(y_batch)
        del batch, res

        if config.scheduler:
            scheduler.step()

        if config.debug:
            k += 1
            if k > 5:
                break

    model.eval()

    ########## VALIDATION ###############
    with torch.no_grad():
        for batch in (valid_loader):
            X_batch = batch[0].to(config.device)
            y_batch = batch[1].to(config.device)

            res = model.forward(X_batch)
            loss = loss_f(res.float(), y_batch)
            y_batch_onehot = y_batch

            avg_val_loss += loss * len(y_batch)

            # metrics
            res = res.detach().cpu().sigmoid().numpy()
            y_batch_onehot = y_batch_onehot.unsqueeze(1).detach().cpu().numpy()
            y_batch_onehot = y_batch_onehot.squeeze()

            if predicted_labels_list is None:
                predicted_labels_list = res
                true_labels_list = y_batch_onehot
            else:
                predicted_labels_list = np.concatenate([predicted_labels_list, res], axis=0)
                true_labels_list = np.concatenate([true_labels_list, y_batch_onehot], axis=0)

            del batch, res

            if config.debug:
                k += 1
                if k > 10:
                    break

    torch.cuda.empty_cache()

    avg_train_loss = avg_train_loss / len(dataset_train)
    avg_val_loss = avg_val_loss / len(dataset_test)

    all_predicted_labels = np.vstack(predicted_labels_list)
    all_true_labels = np.vstack(true_labels_list)
    all_true_labels = np.squeeze(all_true_labels)
    mask = (all_true_labels > 0) & (all_true_labels < 1)
    all_true_labels[mask] = 0
    avg_metric = metric(all_true_labels, all_predicted_labels)

    logger.info(f'epoch: {epoch_i}')

    logger.info("loss_train: %0.4f| loss_valid: %0.4f|" % (avg_train_loss, avg_val_loss))
    for m in avg_metric:
        logger.info(f"metric {m} : {avg_metric[m]:.<5g}")

    elapsed_time = time.time() - start
    hours = int(elapsed_time // 3600)
    minutes = int((elapsed_time % 3600) // 60)
    seconds = int(elapsed_time % 60)
    logger.info(f"Elapsed time: {hours:02d}:{minutes:02d}:{seconds:02d}")

    if epoch_i % 4 == 0:
        torch.save(model, f'{path_save}/model_{config.model_name}_ep_{epoch_i}.pt')

    torch.save(model, f'{path_save}/model_{config.model_name}_last_version.pt')

2023-09-14 16:29:48| Folder with experiment- ../experiment\14_September_2023_16_29
2023-09-14 16:29:48| ----------params----------
2023-09-14 16:29:48| debug: False
2023-09-14 16:29:48| seed: 1771
2023-09-14 16:29:48| path_to_files_base: ../data
2023-09-14 16:29:48| batch_size: 16
2023-09-14 16:29:48| num_workers: 0
2023-09-14 16:29:48| optimizer_lr: 0.006
2023-09-14 16:29:48| optimizer_wd: 0
2023-09-14 16:29:48| scheduler: CosineAnnealingWarmRestarts
2023-09-14 16:29:48| model_name: tf_efficientnet_b0
2023-09-14 16:29:48| metric: custom
2023-09-14 16:29:48| loss_f: nn.BCEWithLogitsLoss()
2023-09-14 16:29:48| optimizer: Adam
2023-09-14 16:29:48| epochs: 16
2023-09-14 16:29:48| Set seed: 1771
2023-09-14 16:29:48| Size df_train- 12326
2023-09-14 16:29:48| Size df_test- 3082
2023-09-14 16:29:48| New experiment
2023-09-14 16:29:48| Scheduler - CosineAnnealingWarmRestarts
2023-09-14 16:29:48| Starting train. Model - tf_efficientnet_b0
2023-09-14 16:29:48| ---------------------epoch:1/16---------------------
2023-09-14 16:45:53| epoch: 1
2023-09-14 16:45:53| loss_train: 0.0235| loss_valid: 0.0209|
2023-09-14 16:45:53| metric val_RMAP : 0.237207
2023-09-14 16:45:53| metric CMAP_5 : 0.435031
2023-09-14 16:45:53| Elapsed time: 00:16:04
2023-09-14 16:45:53| ---------------------epoch:2/16---------------------
2023-09-14 17:00:42| epoch: 2
2023-09-14 17:00:42| loss_train: 0.0179| loss_valid: 0.0173|
2023-09-14 17:00:42| metric val_RMAP : 0.375135
2023-09-14 17:00:42| metric CMAP_5 : 0.536815
2023-09-14 17:00:42| Elapsed time: 00:14:49
2023-09-14 17:00:42| ---------------------epoch:3/16---------------------
2023-09-14 17:14:51| epoch: 3
2023-09-14 17:14:51| loss_train: 0.0153| loss_valid: 0.0127|
2023-09-14 17:14:51| metric val_RMAP : 0.552214
2023-09-14 17:14:51| metric CMAP_5 : 0.676871
2023-09-14 17:14:51| Elapsed time: 00:14:09
2023-09-14 17:14:51| ---------------------epoch:4/16---------------------
2023-09-14 17:29:06| epoch: 4
2023-09-14 17:29:06| loss_train: 0.0150| loss_valid: 0.0157|
2023-09-14 17:29:06| metric val_RMAP : 0.442661
2023-09-14 17:29:06| metric CMAP_5 : 0.576986
2023-09-14 17:29:06| Elapsed time: 00:14:14
2023-09-14 17:29:06| ---------------------epoch:5/16---------------------
2023-09-14 17:43:21| epoch: 5
2023-09-14 17:43:21| loss_train: 0.0139| loss_valid: 0.0128|
2023-09-14 17:43:21| metric val_RMAP : 0.571774
2023-09-14 17:43:21| metric CMAP_5 : 0.682275
2023-09-14 17:43:21| Elapsed time: 00:14:15
2023-09-14 17:43:22| ---------------------epoch:6/16---------------------
2023-09-14 17:57:47| epoch: 6
2023-09-14 17:57:47| loss_train: 0.0112| loss_valid: 0.0104|
2023-09-14 17:57:47| metric val_RMAP : 0.654422
2023-09-14 17:57:47| metric CMAP_5 : 0.755792
2023-09-14 17:57:47| Elapsed time: 00:14:25
2023-09-14 17:57:47| ---------------------epoch:7/16---------------------
2023-09-14 18:11:59| epoch: 7
2023-09-14 18:11:59| loss_train: 0.0112| loss_valid: 0.0151|
2023-09-14 18:11:59| metric val_RMAP : 0.485852
2023-09-14 18:11:59| metric CMAP_5 : 0.616168
2023-09-14 18:11:59| Elapsed time: 00:14:11
2023-09-14 18:11:59| ---------------------epoch:8/16---------------------
2023-09-14 18:26:18| epoch: 8
2023-09-14 18:26:18| loss_train: 0.0129| loss_valid: 0.0129|
2023-09-14 18:26:18| metric val_RMAP : 0.565051
2023-09-14 18:26:18| metric CMAP_5 : 0.680968
2023-09-14 18:26:18| Elapsed time: 00:14:18
2023-09-14 18:26:18| ---------------------epoch:9/16---------------------
2023-09-14 18:40:34| epoch: 9
2023-09-14 18:40:34| loss_train: 0.0115| loss_valid: 0.0125|
2023-09-14 18:40:34| metric val_RMAP : 0.595036
2023-09-14 18:40:34| metric CMAP_5 : 0.698218
2023-09-14 18:40:34| Elapsed time: 00:14:15
2023-09-14 18:40:34| ---------------------epoch:10/16---------------------
2023-09-14 18:55:33| epoch: 10
2023-09-14 18:55:33| loss_train: 0.0102| loss_valid: 0.0101|
2023-09-14 18:55:33| metric val_RMAP : 0.669803
2023-09-14 18:55:33| metric CMAP_5 : 0.765687
2023-09-14 18:55:33| Elapsed time: 00:14:59
2023-09-14 18:55:33| ---------------------epoch:11/16---------------------
2023-09-14 19:09:52| epoch: 11
2023-09-14 19:09:52| loss_train: 0.0090| loss_valid: 0.0093|
2023-09-14 19:09:52| metric val_RMAP : 0.704984
2023-09-14 19:09:52| metric CMAP_5 : 0.793987
2023-09-14 19:09:52| Elapsed time: 00:14:19
2023-09-14 19:09:52| ---------------------epoch:12/16---------------------
2023-09-14 19:23:56| epoch: 12
2023-09-14 19:23:56| loss_train: 0.0077| loss_valid: 0.0089|
2023-09-14 19:23:56| metric val_RMAP : 0.725896
2023-09-14 19:23:56| metric CMAP_5 : 0.809235
2023-09-14 19:23:56| Elapsed time: 00:14:03
2023-09-14 19:23:56| ---------------------epoch:13/16---------------------
2023-09-14 19:38:09| epoch: 13
2023-09-14 19:38:09| loss_train: 0.0068| loss_valid: 0.0087|
2023-09-14 19:38:09| metric val_RMAP : 0.723482
2023-09-14 19:38:09| metric CMAP_5 : 0.813935
2023-09-14 19:38:09| Elapsed time: 00:14:13
2023-09-14 19:38:09| ---------------------epoch:14/16---------------------
2023-09-14 19:52:20| epoch: 14
2023-09-14 19:52:20| loss_train: 0.0098| loss_valid: 0.0121|
2023-09-14 19:52:20| metric val_RMAP : 0.61148
2023-09-14 19:52:20| metric CMAP_5 : 0.711797
2023-09-14 19:52:20| Elapsed time: 00:14:10
2023-09-14 19:52:20| ---------------------epoch:15/16---------------------
2023-09-14 20:06:39| epoch: 15
2023-09-14 20:06:39| loss_train: 0.0102| loss_valid: 0.0109|
2023-09-14 20:06:39| metric val_RMAP : 0.652748
2023-09-14 20:06:39| metric CMAP_5 : 0.747104
2023-09-14 20:06:39| Elapsed time: 00:14:18
2023-09-14 20:06:39| ---------------------epoch:16/16---------------------
2023-09-14 20:20:21| epoch: 16
2023-09-14 20:20:21| loss_train: 0.0096| loss_valid: 0.0105|
2023-09-14 20:20:21| metric val_RMAP : 0.671708
2023-09-14 20:20:21| metric CMAP_5 : 0.764222
2023-09-14 20:20:21| Elapsed time: 00:13:42
